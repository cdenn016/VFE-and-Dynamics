
# ============================================================================
# EDIT THESE DEFAULTS TO RUN WITHOUT COMMAND-LINE ARGS
# ============================================================================
DEFAULT_FFN_MODE = 'variational_gradient_engine'  # 'learned', 'variational_approx', 'variational_full', 'variational_gradient_engine', or None
DEFAULT_RUN_ABLATION = False  # Set True to run all four modes
DEFAULT_ENABLE_SIGMA_PHI = True   # Set True to enable learning Σ and φ (full geometric learning!)
# ============================================================================



PUBLICATION_CONFIG = {
    # Model architecture (minimal but meaningful)
    'vocab_size': 512,        # Will be set by actual char vocab (typically ~100 chars in WikiText-2)
    'embed_dim': 21,          # K=11 (ODD - required for SO(3) irreps!)
    'n_layers': 3,            # Depth for non-trivial learning
    'hidden_dim': 84,         # 4×embed_dim
    'max_seq_len': 32,        # N=32 (key: enough for patterns!)

    # Gauge transformer parameters
    'kappa_beta': 1,
    'epsilon': 1e-8,
    'pos_encoding_mode': 'learned',
    'evolve_sigma': False,  # Auto-enabled for variational_gradient_engine mode
    'evolve_phi': False,    # Keep simple for publication
    'tie_embeddings': True,

    # Attention pattern (full for small N=32)
    'attention_pattern': 'full',
    'attention_window': 32,
    'attention_global_tokens': 0,

    # Variational FFN parameters (will be varied in ablation study)
    'ffn_mode': 'variational_gradient_engine',        # Default: will be overridden in ablation
    'ffn_alpha': 0.2,             # Prior weight (balanced)
    'ffn_tau_eff': 1.0,           # Temperature
    'ffn_kappa': 1.0,             # Softmax temperature
    'ffn_n_iterations': 1,        # Single inference step per forward pass
    'ffn_learnable_lr': True,     # Learn step size for variational descent

    # Sparse variational inference (full for N=32)
    'ffn_pattern': 'full',
    'ffn_window': 32,

    # Training (optimized for convergence)
    'batch_size': 8,             # Larger batches for stability
    'max_steps': 50,              # Adjusted for ~2 hour runtime

    # Natural gradient learning rates (balanced for fast convergence)
    'mu_lr':    0.25,                # Belief means
    'sigma_lr': 0.05,            # Belief covariances
    'phi_lr':   0.1,               # Gauge transformations
    'ffn_lr':   0.25,              # FFN parameters (if learned mode)

    'warmup_steps': 4,          # Gradual warmup for stability

    # Free energy weights (balanced gauge-theoretic learning)
    'alpha': 0.2,                # Self-consistency regularization
    'beta': 1,                  # Belief alignment (key gauge term)
    'lambda_gamma': 1,          # Model alignment (disabled)
    'kappa_gamma': 1.0,         # Temperature for γ_ij coupling

    # Regularization (light for small model)
    'weight_decay': 0.01,
    'dropout': 0.1,
    'grad_clip': 0.0,

    # Logging (frequent for publication plots)
    'log_interval': 1,
    'eval_interval': 2,          # Eval every 50 steps
    'checkpoint_interval': 15,
    'patience': 3,               # Early stopping patience

    # Irrep structure (for K=11)
    'irrep_spec': [
        ('ℓ0', 5, 1),    # 5 dimensions (scalars)
        ('ℓ1', 2, 3),    # 6 dimensions (vectors)
        ('ℓ2', 1, 5),    # 11 dimensions (tensors)
        # Total: 5 + 6 + 11= 21 ✓
    ],
}